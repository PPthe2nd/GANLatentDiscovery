{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 12.00 GiB total capacity; 9.11 GiB already allocated; 0 bytes free; 9.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\GANLatentDiscovery\\run_train.py:106\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m     inspect_all_directions(\n\u001b[0;32m    101\u001b[0m         G, deformator, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharts_s\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m params\u001b[38;5;241m.\u001b[39mshift_scale))),\n\u001b[0;32m    102\u001b[0m         zs\u001b[38;5;241m=\u001b[39mz, shifts_r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m params\u001b[38;5;241m.\u001b[39mshift_scale)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\GANLatentDiscovery\\run_train.py:88\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m params\u001b[38;5;241m.\u001b[39mmax_latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(deformator\u001b[38;5;241m.\u001b[39mout_dim)\n\u001b[0;32m     87\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(params, out_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mout)\n\u001b[1;32m---> 88\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeformator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m save_results_charts(G, deformator, params, trainer\u001b[38;5;241m.\u001b[39mlog_dir)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\GANLatentDiscovery\\trainer.py:209\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, G, deformator, shift_predictor, multi_gpu)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m G(z)\n\u001b[1;32m--> 209\u001b[0m     imgs_shifted \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_shifted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m logits, shift_prediction \u001b[38;5;241m=\u001b[39m shift_predictor(imgs, imgs_shifted)\n\u001b[0;32m    212\u001b[0m logit_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp\u001b[38;5;241m.\u001b[39mlabel_weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy(logits, target_indices)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\GANLatentDiscovery\\models\\gan_load.py:36\u001b[0m, in \u001b[0;36mStyleGanXL.gen_shifted\u001b[1;34m(self, z, shift)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_shifted\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, shift):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\GANLatentDiscovery\\models\\gan_load.py:33\u001b[0m, in \u001b[0;36mStyleGanXL.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     31\u001b[0m w_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(w_avg,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     32\u001b[0m w_pca \u001b[38;5;241m=\u001b[39m w_avg \u001b[38;5;241m+\u001b[39m (w \u001b[38;5;241m-\u001b[39m w_avg)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle_gan_xl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\environment\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m<string>:660\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, ws, **layer_kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\environment\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m<string>:525\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x, w, noise_mode, force_fp32, update_emas)\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\GANLatentDiscovery\\../stylegan_xl\\torch_utils\\misc.py:103\u001b[0m, in \u001b[0;36mprofiled_function.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m<string>:61\u001b[0m, in \u001b[0;36mmodulated_conv2d\u001b[1;34m(x, w, s, demodulate, padding, input_gain)\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 12.00 GiB total capacity; 9.11 GiB already allocated; 0 bytes free; 9.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%run -i run_train.py \\\n",
    "    --gan_type StyleGanXL \\\n",
    "    --deformator ortho \\\n",
    "    --out ../BrainLatentDiscovery/res/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torch_tools.visualization import to_image\n",
    "from visualization import interpolate\n",
    "from loading import load_from_dir\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "deformator, G, shift_predictor = load_from_dir(\n",
    "    './models/pretrained/deformators/SN_MNIST/',\n",
    "    G_weights='./models/pretrained/generators/SN_MNIST/')\n",
    "\n",
    "# deformator, G, shift_predictor = load_from_dir(\n",
    "#     './models/pretrained/deformators/SN_Anime/',\n",
    "#     G_weights='./models/pretrained/generators/SN_Anime/')\n",
    "\n",
    "# deformator, G, shift_predictor = load_from_dir(\n",
    "#     './models/pretrained/deformators/BigGAN/',\n",
    "#     G_weights='./models/pretrained/generators/BigGAN/G_ema.pth')\n",
    "\n",
    "# deformator, G, shift_predictor = load_from_dir(\n",
    "#     './models/pretrained/deformators/ProgGAN/',\n",
    "#     G_weights='./models/pretrained/generators/ProgGAN/100_celeb_hq_network-snapshot-010403.pth')\n",
    "\n",
    "# deformator, G, shift_predictor = load_from_dir(\n",
    "#     './models/pretrained/deformators/StyleGAN2/',\n",
    "#     G_weights='./models/pretrained/generators/StyleGAN2/stylegan2-ffhq-config-f.pt')\n",
    "\n",
    "discovered_annotation = ''\n",
    "for d in deformator.annotation.items():\n",
    "    discovered_annotation += '{}: {}\\n'.format(d[0], d[1])\n",
    "print('human-annotated directions:\\n' + discovered_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_conditional\n",
    "\n",
    "rows = 8\n",
    "plt.figure(figsize=(5, rows), dpi=250)\n",
    "\n",
    "# set desired class for conditional GAN\n",
    "if is_conditional(G):\n",
    "    G.set_classes(12)\n",
    "\n",
    "annotated = list(deformator.annotation.values())\n",
    "inspection_dim = annotated[0]\n",
    "zs = torch.randn([rows, G.dim_z] if type(G.dim_z) == int else [rows] + G.dim_z, device='cuda')\n",
    "\n",
    "\n",
    "for z, i in zip(zs, range(rows)):\n",
    "    interpolation_deformed = interpolate(\n",
    "        G, z.unsqueeze(0),\n",
    "        shifts_r=16,\n",
    "        shifts_count=3,\n",
    "        dim=inspection_dim,\n",
    "        deformator=deformator,\n",
    "        with_central_border=True)\n",
    "\n",
    "    plt.subplot(rows, 1, i + 1)\n",
    "    plt.axis('off')\n",
    "    grid = make_grid(interpolation_deformed, nrow=11, padding=1, pad_value=0.0)\n",
    "    grid = torch.clamp(grid, -1, 1)\n",
    "\n",
    "    plt.imshow(to_image(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
